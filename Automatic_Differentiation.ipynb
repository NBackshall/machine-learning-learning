{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NBackshall/machine-learning-learning/blob/main/Automatic_Differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic differentiation\n",
        "Automatic differentiation (AD), also called algorithmic differentiation or simply \"autodiff\", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs.\n",
        "\n",
        "Autodiff is a solution to the problems encountered with Numerical Differentiation (which has a complxity of $O(n)$) and Symbolic Differentiation (which can suffer from expression swell where $f'(x)$ may be exponentially longer than $f(x)$ caused by rules like the product rule; also, Symbolic requires closed-form expressions - this limits ability to use control flow mechanisms: conditionals, loops and recursion). Autodiff operates directly on program of interest (whilst achieving the same accuracy as symbolic), rather than obtaining an expression the goal of autodiff is to calculate its numerical value."
      ],
      "metadata": {
        "id": "ynULer-vKl9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "uNzTQsAE-Ab4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.).\n",
        "\n",
        "Additionally, AD utilises the decompositin of differentials provided by the chain rule:\n",
        "\n",
        "$y=f(g(h(x)))=f(g(h(w_0)))=f(g(w_1))=f(w_2)=w_3$\n",
        "\n",
        "$w_0=x$\n",
        "\n",
        "$w_1=h(w_o)$\n",
        "\n",
        "$w_2=g(w_1)$\n",
        "\n",
        "$w_3=f(w_2)=y$\n",
        "\n",
        "Using the above, the chain rule gives:\n",
        "\n",
        "$\\frac{dy}{dx}=\\frac{dy}{dw_2}\\frac{dw_2}{dw_1}\\frac{dw_1}{dx}$\n",
        "\n",
        "$\\frac{dy}{dx}=\\frac{df(w_2)}{dw_2}\\frac{dg(w_1)}{dw_1}\\frac{dh(w_0)}{dw_0}$\n",
        "\n",
        "Forward accumulation (or mode) and backward accumulation (or mode) changes which direction you travers this chain rule (forward: left to right; backward: right to left).\n",
        "\n",
        "1.   Forward mode: $\\frac{dw_i}{dx}=\\frac{dw_i}{dw_{i-1}}\\frac{dw_{i-1}}{dx}$ (for above example $w_3=y$)\n",
        "2.   Backward mode: $\\frac{dy}{dw_i}=\\frac{dy}{dw_{i+1}}\\frac{dw_{i+1}}{dw_i}$ (for above example $w_0=x$)\n",
        "\n",
        "## Forward mode\n",
        "In forward mode AD, one first fixes the independent variable w.r.t. which differentiation is performed and computes the derivative of each sub-expression recursively. This involves repeatedly substituting the derivative of the inner functions in the chain rule:\n",
        "\n",
        "$\\frac{∂y}{∂x}=\\frac{∂y}{∂w_{n-1}}\\frac{∂w_{n-1}}{∂x}$\n",
        "\n",
        "$\\frac{∂y}{∂x}=\\frac{∂y}{∂w_{n-1}}(\\frac{∂w_{n-1}}{∂w_{n-2}}\\frac{∂w_{n-2}}{∂x})$ and so on...\n",
        "\n",
        "Forward mode is easier to implement as the each derivitive is calculated before it is required. Each variable $w$ is stored in a tuple with it's derivitive, $ẇ=\\frac{∂w}{∂x}$.\n",
        "\n",
        "As an example:\n",
        "\n",
        "$y=f(x_1,x_2)$\n",
        "\n",
        "$y=x_1x_2+sin(x_1)$\n",
        "\n",
        "$y=w_1w_2+sin(w_1)$\n",
        "\n",
        "$y=w_3+w_4$\n",
        "\n",
        "$y=w_5$\n",
        "\n",
        "The choice of the independent variable to which differentiation is performed affects the seed values $\\dot{w}_1$ and $\\dot{w}_2$. Given interest in the derivative of this function with respect to $x_1$, the seed values should be set to:\n",
        "\n",
        "$ẇ_1=\\frac{∂x_1}{∂x_1}=1$\n",
        "\n",
        "\n",
        "$\\dot{w}_2=\\frac{∂x_2}{∂x_1}=0$\n",
        "\n",
        "With these seed values:\n",
        "\n",
        "$(w_1,\\dot{w}_1)=(w_1,1)$\n",
        "\n",
        "$(w_2,\\dot{w}_2)=(w_2,0)$\n",
        "\n",
        "$(w_3,\\dot{w}_3)=(w_1w_2,w_1\\dot{w_2}+\\dot{w_1}w_2)$\n",
        "\n",
        "$(w_4,\\dot{w}_4)=(sin(x_1),sin(x_1)\\dot{w}_1)$\n",
        "\n",
        "$(w_5,\\dot{w}_5)=(w_3+w_4,\\dot{w_3}+\\dot{w_4})$\n",
        "\n",
        "In this case, to the gradient requires computing derivitive w.r.t. $x_1$ and $x_2$; therefore, two sweeps are required switching the seed falues from $[1,0]$ to $[0,1]$."
      ],
      "metadata": {
        "id": "t6cSCsPqPH8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dual numbers\n",
        "In order to implment Forward mode AD dual numbers can be used with operation overloading. This is shown above where each variable $w$ is augmented with its derivitive, $\\langle{w,\\dot{w}}\\rangle{}$.\n",
        "\n",
        "The new arithmetic consists of ordered pairs, elements written $\\langle x, \\dot{x} \\rangle$, with ordinary arithmetics on the first component, and first order differentiation arithmetic on the second component, as described above. For instance, multiplication:\n",
        "\n",
        "$\\langle{u,\\dot{u}}\\rangle{}\\times\\langle{v,\\dot{v}}\\rangle{}=\\langle{uv,u\\dot{v}+v\\dot{u}}\\rangle{}$\n",
        "\n",
        "And also $sin$:\n",
        "\n",
        "$sin\\langle{u,\\dot{u}}\\rangle{}=\\langle{sin(u),\\dot{u}\\times{}cos(u)}\\rangle{}$"
      ],
      "metadata": {
        "id": "4IVRlHoopnIe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9WtGR7gkKfP_"
      },
      "outputs": [],
      "source": [
        "class DualNumber:\n",
        "  def __init__(self, value, dvalue):\n",
        "    self.value = value\n",
        "    self.dvalue = dvalue\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.value) + \" + \" + str(self.dvalue) + \"ε\"\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return DualNumber(\n",
        "        self.value + other.value,\n",
        "        self.dvalue + other.dvalue\n",
        "    )\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    return DualNumber(\n",
        "        self.value - other.value,\n",
        "        self.dvalue - other.dvalue\n",
        "    )\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    return DualNumber(\n",
        "        self.value * other.value,\n",
        "        self.value * other.dvalue + self.dvalue * other.value\n",
        "    )\n",
        "\n",
        "  def __truediv__(self,other):\n",
        "    return DualNumber(\n",
        "        self.value / other.value,\n",
        "        (other.value * self.dvalue - self.value * other.dvalue) / other.value**2\n",
        "    )\n",
        "  \n",
        "  def __pow__(self, other):\n",
        "    value = self.value ** other.value\n",
        "    dvalue = value * (self.dvalue * (other.value / self.value) + other.dvalue * math.log(self.value))\n",
        "    return DualNumber(value,dvalue)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Dual number using two equations: $u=x^2$ and $v=5x$"
      ],
      "metadata": {
        "id": "rU51uZinujYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 1.2\n",
        "u = DualNumber(x**2,2*x)\n",
        "v = DualNumber(5*x,5)\n",
        "\n",
        "test_value = ((x**2)**(5*x) + (x**2)*(5*x)) / ((x**2) - (5*x))\n",
        "test_dvalue = (15*x**2 + (x**2)**(5*x) * (5*math.log(x**2) + 10)) / (\n",
        "    x**2 - 5*x) - ((2*x - 5) * (5*x**3 + (x**2)**(5*x))) / (x**2 - 5*x)**2\n",
        "dual_number = (u**v + u*v) / (u - v)\n",
        "\n",
        "print(\"The correct answer is {} + {}ε \\nDualNumber found      {}\".format(test_value, test_dvalue, dual_number))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0NjAjLOt7yi",
        "outputId": "ef6e093f-910e-4429-cf0b-d8b0ed5258de"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The correct answer is -3.8500220281263147 + -25.659412357895754ε \n",
            "DualNumber found      -3.8500220281263147 + -25.65941235789575ε\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to be able to handle mathematical functions such as sin and exp."
      ],
      "metadata": {
        "id": "MU8ylFhsC4PM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sin(x: DualNumber):\n",
        "  return DualNumber(math.sin(x.value), x.dvalue * math.cos(x.value))\n",
        "\n",
        "def cos(x: DualNumber):\n",
        "  return DualNumber(math.cos(x.value), - x.dvalue * math.sin(x.value))\n",
        "\n",
        "def exp(x: DualNumber):\n",
        "  return DualNumber(math.exp(x.value), x.dvalue * math.exp(x.value))"
      ],
      "metadata": {
        "id": "qnVHexE6uxfu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing for $y=f(x)=sin^{cos(x^3)}(2x)$ where $x=1.2$ (from WolframAlpha.com):\n",
        "\n",
        "$f(1.2)=1.06335$\n",
        "\n",
        "$f'(1.2)=2.14361$"
      ],
      "metadata": {
        "id": "Z8FsKiODIO7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 1.2\n",
        "w_1 = DualNumber(x,1)\n",
        "w_2 = DualNumber(2,0)*w_1\n",
        "w_3 = w_1**DualNumber(3,0)\n",
        "w_4 = sin(w_2)\n",
        "w_5 = cos(w_3)\n",
        "w_6 = w_4**w_5\n",
        "\n",
        "print(\"w_1 = {}\\nw_2 = {}\\nw_3 = {}\\nw_4 = {}\\nw_5 = {}\\nw_6 = {}, \".format(w_1,w_2,w_3,w_4,w_5,w_6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyJT1FTtvt3E",
        "outputId": "95309687-434e-4875-83de-9404844e50e2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_1 = 1.2 + 1ε\n",
            "w_2 = 2.4 + 2.0ε\n",
            "w_3 = 1.7279999999999998 + 4.319999999999999ε\n",
            "w_4 = 0.675463180551151 + -1.4747874310824909ε\n",
            "w_5 = -0.15655697721737202 + -4.266729772345185ε\n",
            "w_6 = 1.0633519842136765 + 2.1436132251798043ε, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing for $y=f(x)=e^{sin^{cos(x^3)}(2x)}$ where $x=1.2$ (from WolframAlpha.com):\n",
        "\n",
        "$f(1.2)=2.89606$\n",
        "\n",
        "$f'(1.2)=6.20804$"
      ],
      "metadata": {
        "id": "1qqEWlaxJRGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_7 = exp(w_6)\n",
        "print(\"w_7 = {}\".format(w_7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39D3-hnYDi_7",
        "outputId": "5aa964fb-111d-4cfd-c5da-4c82e9e11dfe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_7 = 2.8960622927327844 + 6.2080374316465425ε\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward mode\n",
        "\n",
        "A really helpful article: https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation\n",
        "\n",
        "The implementation simplicity of forward-mode AD comes with a big disadvantage, which becomes evident when we want to calculate both ∂z/∂x and ∂z/∂y. In forward-mode AD, doing so requires seeding with dx = 1 and dy = 0, running the program, then seeding with dx = 0 and dy = 1 and running the program again. In effect, the cost of the method scales linearly as O(n) where n is the number of input variables. This would be very costly if we wanted to calculate the gradient of a large complicated function of many variables, which happens surprisingly often in practice.\n",
        "\n",
        "One way is to parse the original program and then generate an adjoint program that calculates the derivatives. However this can be complex to implement, especially with certain programming languages.\n",
        "\n",
        "A simpler way is to do this dynamically: construct a full graph that represents our original expression as the program runs. The goal is to get something akin to a dependency graph.\n",
        "\n",
        "By default, a node is created without any children. However, whenever a new expression u is built out of existing nodes $w_i$, the new expression $u$ registers itself as a child of each of its dependencies $w_i$. During the child registration, it will also save its contributing weight $\\frac{∂w_i}{∂u}$ which will be used later to compute the gradients. \n"
      ],
      "metadata": {
        "id": "7tDvLDCMJ4Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Var:\n",
        "  def __init__(self, value):\n",
        "    self.value = value\n",
        "    self.children = [] # [(weight, Var)]\n",
        "    self.grad_value = None # None used to check if grad_value calculated\n",
        "\n",
        "  def gradient(self):\n",
        "    # recurse iff grad_value == None\n",
        "    if self.grad_value is None:\n",
        "      self.grad_value = sum(weight * var.gradient() for weight, var in self.children)\n",
        "    return self.grad_value\n",
        "\n",
        "  def set_gradient(self, value):\n",
        "    self.grad_value = value\n",
        "\n",
        "  def __add__(self, other):\n",
        "    z = Var(self.value + other.value)\n",
        "    self.children.append((1, z))\n",
        "    other.children.append((1, z))\n",
        "    return z\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    z = Var(self.value - other.value)\n",
        "    self.children.append((1, z))\n",
        "    other.children.append((-1, z))\n",
        "    return z\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    # the weight is what the child's gradient is multiplied by as a contribution\n",
        "    # to the sum used to calculate the parent gradient.\n",
        "    z = Var(self.value * other.value) # z is the new child\n",
        "    self.children.append((other.value, z)) # weight = ∂z/∂self = other.value\n",
        "    other.children.append((self.value, z)) # weight = ∂z/∂other = self.value\n",
        "    return z\n",
        "\n",
        "  def __truediv__(self, other):\n",
        "    z = Var(self.value / other.value)\n",
        "    self.children.append((1/other.value, z))\n",
        "    other.children.append((-self.value / other.value**2, z))\n",
        "    return z\n",
        "\n",
        "  def __pow__(self, other):\n",
        "    z = Var(self.value ** other.value)\n",
        "    self.children.append((self.value ** (other.value - 1), z))\n",
        "    other.children.append((self.value ** other.value * math.log(self.value), z))\n",
        "    return z\n",
        "\n",
        "  def __str__(self):\n",
        "    if self.grad_value is None:\n",
        "      return \"value={}, {} child(ren), grad_val={}\".format(self.value, len(self.children), \"None\")\n",
        "    else:\n",
        "      return \"value={}, {} child(ren), grad_val={}\".format(self.value, len(self.children), self.grad_value)"
      ],
      "metadata": {
        "id": "rwiJx-EkHqda"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u_1 = Var(0.5)\n",
        "u_2 = Var(4.2)\n",
        "u_3 = u_1 / u_2\n",
        "u_4 = u_1 ** u_2\n",
        "u_5 = u_3 + u_4\n",
        "\n",
        "\n",
        "print(\"u_1 = {}\\nu_2 = {}\\nu_3 = {}\\nu_4 = {}\\nu_5 = {}\".format(u_1,u_2,u_3,u_4,u_5))\n",
        "\n",
        "u_5.set_gradient(1)\n",
        "\n",
        "print(\"---\")\n",
        "\n",
        "print(\"u_1 = {}\\nu_2 = {}\\nu_3 = {}\\nu_4 = {}\\nu_5 = {}\".format(u_1,u_2,u_3,u_4,u_5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cAYL1nzznKp",
        "outputId": "72fd79d6-4fe5-4c69-a90b-38fbf313243f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u_1 = value=0.5, 2 child(ren), grad_val=None\n",
            "u_2 = value=4.2, 2 child(ren), grad_val=None\n",
            "u_3 = value=0.11904761904761904, 1 child(ren), grad_val=None\n",
            "u_4 = value=0.05440941020600775, 1 child(ren), grad_val=None\n",
            "u_5 = value=0.1734570292536268, 0 child(ren), grad_val=None\n",
            "---\n",
            "u_1 = value=0.5, 2 child(ren), grad_val=None\n",
            "u_2 = value=4.2, 2 child(ren), grad_val=None\n",
            "u_3 = value=0.11904761904761904, 1 child(ren), grad_val=None\n",
            "u_4 = value=0.05440941020600775, 1 child(ren), grad_val=None\n",
            "u_5 = value=0.1734570292536268, 0 child(ren), grad_val=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add some mathematical function like in Forward Mode AD."
      ],
      "metadata": {
        "id": "J5XpE_u0bnRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def b_sin(x):\n",
        "  z = Var(math.sin(x.value))\n",
        "  x.children.append((math.cos(x.value), z))\n",
        "  return z\n",
        "\n",
        "def b_cos(x):\n",
        "  z = Var(math.cos(x.value))\n",
        "  x.children.append((-math.sin(x.value), z))\n",
        "  return z\n",
        "\n",
        "def b_exp(x):\n",
        "  z = Var(math.exp(x.value))\n",
        "  x.children.append((math.exp(x.value), z))\n",
        "  return z"
      ],
      "metadata": {
        "id": "ZFpyyDE71KGE"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test, following example from the following forum accepted answer: https://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation"
      ],
      "metadata": {
        "id": "ltxW7Wesbduu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 2\n",
        "y = 3\n",
        "w_1 = DualNumber(x,1)\n",
        "w_2 = DualNumber(y,1)\n",
        "w_3 = w_1 * w_2\n",
        "w_4 = sin(w_1)\n",
        "w_5 = w_3 + w_4\n",
        "\n",
        "print(\"w_1 = {}\\nw_2 = {}\\nw_3 = {}\\nw_4 = {}\\nw_5 = {}\".format(w_1,w_2,w_3,w_4,w_5))\n",
        "\n",
        "print(\"---\")\n",
        "\n",
        "u_1 = Var(w_1.value)\n",
        "u_2 = Var(w_2.value)\n",
        "u_3 = u_1 * u_2\n",
        "u_4 = b_sin(u_1)\n",
        "u_5 = u_3 + u_4\n",
        "\n",
        "\n",
        "print(\"u_1 = {}\\nu_2 = {}\\nu_3 = {}\\nu_4 = {}\\nu_5 = {}\".format(u_1,u_2,u_3,u_4,u_5))\n",
        "\n",
        "# set gradient of dz/dz\n",
        "u_5.set_gradient(1)\n",
        "\n",
        "u_1.gradient() # calculate gradient of dz/dx\n",
        "u_2.gradient() # calculate gradient of dz/dy\n",
        "\n",
        "print(\"---\")\n",
        "\n",
        "print(\"u_1 = {}\\nu_2 = {}\\nu_3 = {}\\nu_4 = {}\\nu_5 = {}\".format(u_1,u_2,u_3,u_4,u_5))\n",
        "\n",
        "print(\"---\")\n",
        "\n",
        "print(\"dz/dx = {}\".format(u_1.gradient()))\n",
        "print(\"dz/dy = {}\".format(u_2.gradient()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWzhjgdMYB87",
        "outputId": "43035288-3409-4035-d30e-ad31c1e60296"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_1 = 2 + 1ε\n",
            "w_2 = 3 + 1ε\n",
            "w_3 = 6 + 5ε\n",
            "w_4 = 0.9092974268256817 + -0.4161468365471424ε\n",
            "w_5 = 6.909297426825682 + 4.583853163452858ε\n",
            "---\n",
            "u_1 = value=2, 2 child(ren), grad_val=None\n",
            "u_2 = value=3, 1 child(ren), grad_val=None\n",
            "u_3 = value=6, 1 child(ren), grad_val=None\n",
            "u_4 = value=0.9092974268256817, 1 child(ren), grad_val=None\n",
            "u_5 = value=6.909297426825682, 0 child(ren), grad_val=None\n",
            "---\n",
            "u_1 = value=2, 2 child(ren), grad_val=2.5838531634528574\n",
            "u_2 = value=3, 1 child(ren), grad_val=2\n",
            "u_3 = value=6, 1 child(ren), grad_val=1\n",
            "u_4 = value=0.9092974268256817, 1 child(ren), grad_val=1\n",
            "u_5 = value=6.909297426825682, 0 child(ren), grad_val=1\n",
            "---\n",
            "dz/dx = 2.5838531634528574\n",
            "dz/dy = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ogkJ8PlpZa09"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}